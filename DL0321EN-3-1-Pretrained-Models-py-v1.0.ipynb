{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "\n",
    "1.  <a href=\"https://#item31\">Import Libraries and Packages</a>\n",
    "2.  <a href=\"https://#item32\">Download Data</a>\n",
    "3.  <a href=\"https://#item33\">Define Global Constants</a>\n",
    "4.  <a href=\"https://#item34\">Construct ImageDataGenerator Instances</a>\n",
    "5.  <a href=\"https://#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-08 17:37:30.529474: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-08 17:37:31.336020: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:\n",
      "2022-10-08 17:37:31.336125: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:\n",
      "2022-10-08 17:37:31.336137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "#from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## get the data\n",
    "#!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now if you check the left directory pane, you should see the zipped file *concrete_data_week3.zip* appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!unzip concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50**\\* error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab.\n",
    "\n",
    "1.  We are obviously dealing with two classes, so *num_classes* is 2.\n",
    "2.  The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3.  We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-08 17:37:34.222943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-08 17:37:34.264352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-08 17:37:34.264565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-08 17:37:34.265576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-08 17:37:34.265763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-08 17:37:34.265916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-08 17:37:34.825282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-08 17:37:34.825490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-08 17:37:34.825656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-08 17:37:34.825797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2835 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.functional.Functional at 0x7ff6187527f0>,\n",
       " <keras.layers.core.dense.Dense at 0x7ff620e423d0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7ff620e42790>,\n",
       " <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x7ff620e42b20>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b2bc220>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b2bc1f0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b2bc850>,\n",
       " <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x7ff61b2a6f70>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7ff61b266b20>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b20c910>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b1f7be0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b20c490>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b21a940>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b21ab20>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b22f940>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b1f79a0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b22fbe0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b266100>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b22fb50>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b1b91c0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b1b95b0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b1f7e80>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b234280>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b1ca8e0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b1c2130>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b1c2fd0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b1d4850>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b1cf6a0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b1d4c70>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b1e80d0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b1e8670>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b1da910>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b1e29a0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b1b95e0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b1caa60>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b21fd00>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b227400>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b2154c0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b2a68e0>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b1ee070>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b1ee3d0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b1f6700>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b1d1b50>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b1eeca0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b1857f0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b185820>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b197790>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b2152e0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b197910>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b215fa0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b197a60>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b1a6070>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b1a6370>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b19bc10>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b197b50>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b197a90>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b1b0fa0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b1a9bb0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b1b0f40>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b13da00>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b13d8b0>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b1510a0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b151400>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b144580>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b14a970>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b15f760>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b139c70>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b15fdc0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b16c6d0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b16cb20>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b16c8e0>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b151490>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b1a9580>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b16cd30>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b14adf0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b13dbe0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b1d1fa0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b185880>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b1d1d30>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b21a160>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b172f70>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b0f9070>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b0f93d0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b0fc0a0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b0f7820>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b108400>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b108790>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b1088e0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b11a730>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b0f75b0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b11a7c0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b19be50>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b11ab20>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b127040>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b127280>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b11fdf0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b11aac0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b1326d0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b132f10>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b12ba60>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b132eb0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b0c1820>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b0c1610>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b0d3070>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b0d3370>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b0c7610>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b0cd910>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b0e1730>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b0dbe20>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b0e1f10>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b127fa0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b132c10>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b11fa60>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b0fa370>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b172790>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b11fa90>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b108190>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b102730>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b0e8bb0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b0ba4f0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b0ec0d0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b1ee310>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b0ecf40>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b0f12e0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b0f1310>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b0ed970>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b0efaf0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b07e700>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b07ed60>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b07ef10>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b08b0a0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b08b400>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b08b850>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b09f070>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b09f370>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b090640>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b096be0>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b0ab700>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b090e80>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b086c10>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b0abd60>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b0371c0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b0379a0>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b04a070>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff624608610>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b050100>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b0a4070>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b037f70>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b090fd0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b050160>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b042cd0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b03ef40>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b037f10>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b11fbb0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b078220>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b172a30>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b0f1bb0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b0e9b20>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b090f10>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b0f7d90>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b0b1af0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b042f40>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b1148b0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b05fb80>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b0579a0>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b102070>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b086730>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b062940>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b05f100>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b05f070>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b108160>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61b06bf40>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b06b160>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff61b06b3d0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61873b5e0>,\n",
       " <keras.layers.merging.add.Add at 0x7ff61b057610>,\n",
       " <keras.layers.core.activation.Activation at 0x7ff61b0e8760>,\n",
       " <keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x7ff61b0e8fd0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 4098      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5141/251737888.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  fit_history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-08 17:37:48.229845: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n",
      "2022-10-08 17:37:59.081594: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.71GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - 306s 950ms/step - loss: 0.0485 - accuracy: 0.9819 - val_loss: 0.0092 - val_accuracy: 0.9985\n",
      "Epoch 2/2\n",
      "301/301 [==============================] - 280s 929ms/step - loss: 0.0071 - accuracy: 0.9984 - val_loss: 0.0058 - val_accuracy: 0.9987\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3\\_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
    "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
    "| 2020-09-18        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "Copyright Â© 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python4CP",
   "language": "python",
   "name": "python4cp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
